{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b12bf7a8",
   "metadata": {},
   "source": [
    "Lazy loading propositions proof from set.mm so we can serve specific theorems without having to load the entire library.\n",
    "\n",
    "http://us.metamath.org/mpeuni/epee.html\n",
    "\n",
    "http://us.metamath.org/mpeuni/opoeALTV.html\n",
    "\n",
    "http://us.metamath.org/mpeuni/elun.html\n",
    "\n",
    "http://us.metamath.org/mpegif/oddm1evenALTV.html\n",
    "\n",
    "http://us.metamath.org/mpegif/oddp1evenALTV.html\n",
    "\n",
    "http://us.metamath.org/mpegif/2evenALTV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8934c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapt_utils import replace_symbols\n",
    "\n",
    "from tree_parser import *\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy\n",
    "\n",
    "import time\n",
    "\n",
    "from adapt_utils import replace_symbols\n",
    "from my_utils import print_proof_props_graph, get_proof_steps, print_ident_proof, print_proof_linear_steps\n",
    "from my_utils import get_proof_steps_graph, print_proof_steps_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aaea85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included 5555695 tokens from set.mm\n",
      "proposition: 16100"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "('Parsing of', ['|-', '.o.', '=', '(', '<.', 'X', ',', 'Y', '>.', '(', 'comp', '`', 'C', ')', 'X', ')'], 'failed')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\UFABC\\Pesquisa\\AutoProof\\Matutor\\Prototype\\mathview\\tree_parser.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file_contents, n, remember_proof_steps)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnon_entails_axioms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStatementParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file_contents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\UFABC\\Pesquisa\\AutoProof\\Matutor\\Prototype\\mathview\\tree_parser.py\u001b[0m in \u001b[0;36mread_file_contents\u001b[1;34m(self, file_contents, n)\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[1;31m# assumptions for the theorems in the block\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mnext_token\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'$e'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_e\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_block\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m             \u001b[1;31m# an axiom --- I don't check uniqueness of labels like I should.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\UFABC\\Pesquisa\\AutoProof\\Matutor\\Prototype\\mathview\\tree_parser.py\u001b[0m in \u001b[0;36mparse_e\u001b[1;34m(self, current_block, file_contents)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m             \u001b[1;31m# Fuck: the parser failed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Parsing of'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmath_symbols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'failed'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m         \u001b[1;31m#replacement_dict={\"VAR\"+f.variable:f.label for f in current_block.f.values()}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m         \u001b[1;31m#tree.replace_values(replacement_dict)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: ('Parsing of', ['|-', '.o.', '=', '(', '<.', 'X', ',', 'Y', '>.', '(', 'comp', '`', 'C', ')', 'X', ')'], 'failed')"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text = file_contents()\n",
    "database = meta_math_database(text,n=20000, remember_proof_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f09199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbba9e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"set.mm\", 'r')\n",
    "f_contents = f.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f92500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "propositions = []\n",
    "propositions_idx = []\n",
    "idx2prop = {}\n",
    "prop2idx = {}\n",
    "\n",
    "for i, token in enumerate(f_contents):\n",
    "    if token == \"$p\":\n",
    "        prop = f_contents[i-1]\n",
    "        propositions_idx.append(i-1)\n",
    "        propositions.append(prop)\n",
    "        idx2prop[i-1] = prop\n",
    "        prop2idx[prop] = i-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa8f9ef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5243177"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop2idx[\"opoeALTV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd7c6c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'opoeALTV'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_contents[5243177]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c87d4e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proof_from_prop(prop):\n",
    "    prop_idx = prop2idx[prop]\n",
    "    \n",
    "    token_id = prop_idx\n",
    "    token = f_contents[token_id]\n",
    "    \n",
    "    start_proof = False\n",
    "    proof_tokens = []\n",
    "    \n",
    "    while token != \"$.\":        \n",
    "        if token == \"$=\":\n",
    "            start_proof = True\n",
    "        \n",
    "        elif start_proof:\n",
    "            proof_tokens.append(token)\n",
    "            \n",
    "        token_id += 1\n",
    "        token = f_contents[token_id]\n",
    "            \n",
    "    return proof_tokens    \n",
    "\n",
    "def split_compressed_proof(list):\n",
    "    assert list[0] == '('\n",
    "    index = 1\n",
    "    while list[index] != ')': index+=1\n",
    "    labels = list[1:index]  # not including index\n",
    "    proof = ''\n",
    "    for part in list[index+1:]: proof+=part\n",
    "    return labels, proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f773d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "proof_tokens = get_proof_from_prop('bitri')\n",
    "#proof_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d751e9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['sylbb', 'sylbbr', 'impbii'], 'ACABCDEFABCDEGH')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, proof = split_compressed_proof(proof_tokens)\n",
    "labels, proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncompress(self, prop, context):\n",
    "\n",
    "    compressed_proof=prop.proof\n",
    "    if compressed_proof[0]!='(': return # don't do anything if not compressed\n",
    "\n",
    "    label_part, proof = split_compressed_proof(compressed_proof)\n",
    "\n",
    "    # construct a list of all of the valid labels and their arities\n",
    "    # we'll need this when expanding Z statements in the compressed proof.\n",
    "    arities = {}\n",
    "    hypothesis_labels = [hyp.label for hyp in prop.hyps]\n",
    "    for label in hypothesis_labels:\n",
    "        arities[label]=0\n",
    "\n",
    "    token_list = hypothesis_labels+label_part\n",
    "    for label in label_part:\n",
    "        if label in self.propositions:\n",
    "            arities[label] = self.propositions[label].arity()\n",
    "        elif label in context.f:\n",
    "            arities[label] = 0 # hypotheses have 0 arity\n",
    "        else:\n",
    "            print(prop.print_details())\n",
    "            print(token_list)\n",
    "            print(proof)\n",
    "            print(label)\n",
    "            raise Exception('not defined')\n",
    "    zpositions = []\n",
    "\n",
    "    proof = proof.upper()\n",
    "    out = []\n",
    "    index = -1\n",
    "    current = 0\n",
    "    for char in proof:\n",
    "        index+=1 # not needed.  kept for reference\n",
    "        num = ord(char)\n",
    "        if num==90: # Z\n",
    "            zpositions.append(len(out)-1) #save the last one as a z-position\n",
    "        elif (85<=num<=89): # Ord('U')=85, Ord('Y')=89\n",
    "            current *= 5\n",
    "            current += num-84\n",
    "        else:  # Ord('A')=65\n",
    "            current = current *20 + num-64\n",
    "\n",
    "            # append to the out list the corresponding stuff\n",
    "            if current<=len(token_list):\n",
    "                out.append(token_list[current-1])\n",
    "            elif current - len(token_list)<=len(zpositions):\n",
    "                # pull a bunch of stuff from the previous position\n",
    "                source_position = zpositions[current - len(token_list)-1]\n",
    "                to_append = [] # the new stuff to append, written backwards\n",
    "                remaining_arity = 1;\n",
    "                while remaining_arity>0:\n",
    "                    if source_position<0:\n",
    "                        print(token_list,zpositions,compressed_proof)\n",
    "                        print(to_append,source_position)\n",
    "                        print(remaining_arity)\n",
    "                        prop.print_details()\n",
    "                        raise Exception('reached start of proof before finishing Z substitution')\n",
    "                    new_token = out[source_position]\n",
    "                    remaining_arity += -1 + arities[new_token]\n",
    "                    to_append.append(new_token)\n",
    "                    source_position-=1\n",
    "                to_append.reverse()\n",
    "                out += to_append\n",
    "            else:\n",
    "                print(len(token_list),len(zpositions), 'current=',current)\n",
    "                print(token_list,proof[index-5:index+1])\n",
    "                raise Exception('Attemped to refer to non-existant token position')\n",
    "\n",
    "            # now reset the counter\n",
    "            current = 0\n",
    "    prop.proof = out # replace the compressed proof with the uncompressed one\n",
    "    prop._uncompressed = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
